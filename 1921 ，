"""
Cyberbullying Detection System - SafeNet Analytics
Complete web application with all features
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json
import sys
import os

# Add local modules to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_processor import TextProcessor
from config import SYSTEM_CONFIG

# Page configuration
st.set_page_config(
    page_title="SafeNet Analytics - Cyberbullying Detection",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown(f"""
<style>
    .main-title {{
        font-size: 2.5rem;
        color: {SYSTEM_CONFIG['colors']['primary']};
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }}
    .section-header {{
        font-size: 1.8rem;
        color: {SYSTEM_CONFIG['colors']['secondary']};
        margin-top: 1.5rem;
        margin-bottom: 1rem;
        border-bottom: 2px solid #e0e0e0;
        padding-bottom: 0.5rem;
    }}
    .metric-card {{
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin: 1rem 0;
        border-left: 5px solid {SYSTEM_CONFIG['colors']['primary']};
    }}
    .risk-low {{ color: #10B981; font-weight: bold; }}
    .risk-medium {{ color: #F59E0B; font-weight: bold; }}
    .risk-high {{ color: #EF4444; font-weight: bold; }}
    .explanation-box {{
        background-color: #F3F4F6;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }}
    .keyword-badge {{
        display: inline-block;
        background-color: #E5E7EB;
        padding: 0.25rem 0.75rem;
        border-radius: 15px;
        margin: 0.25rem;
        font-size: 0.9rem;
    }}
</style>
""", unsafe_allow_html=True)

class CyberbullyingDetector:
    """Main detection engine with ethical considerations"""
    
    def __init__(self):
        self.processor = TextProcessor()
        self.initialize_detection_rules()
    
    def initialize_detection_rules(self):
        """Initialize detection rules and patterns"""
        self.offensive_patterns = {
            'direct_insults': [
                'idiot', 'stupid', 'moron', 'dumb', 'fool', 'imbecile',
                'retard', 'loser', 'worthless', 'useless', 'pathetic'
            ],
            'appearance_bullying': [
                'ugly', 'fat', 'skinny', 'disgusting', 'hideous', 'gross'
            ],
            'social_exclusion': [
                'nobody likes', 'everyone hates', 'go away', 'get lost',
                'leave us alone', 'you don\'t belong'
            ],
            'threats': [
                'kill', 'die', 'hurt', 'beat', 'punch', 'fight', 'destroy'
            ],
            'hate_speech': [
                'hate', 'racist', 'sexist', 'homophobic', 'bigot',
                'discriminate', 'prejudice'
            ]
        }
        
        self.humor_indicators = [
            'lol', 'lmao', 'haha', 'hehe', 'üòÇ', 'üòÜ', 'üòÑ',
            'just kidding', 'jk', '/s', 'sarcasm', 'joking'
        ]
        
        self.relationship_context = [
            'friend', 'buddy', 'mate', 'bro', 'sis', 'pal',
            'love you', 'miss you', 'bestie'
        ]
    
    def analyze_text(self, text, context=None):
        """Comprehensive text analysis with context awareness"""
        if not text or len(text.strip()) < 3:
            return self._empty_analysis_result()
        
        # Preprocess text
        processed_text = self.processor.preprocess(text)
        
        # Extract features
        features = self.extract_detailed_features(processed_text, text)
        
        # Calculate risk scores
        risk_scores = self.calculate_risk_scores(features, processed_text, context)
        
        # Generate explanation
        explanation = self.generate_explanation(features, risk_scores)
        
        # Determine final classification
        classification = self.classify_content(risk_scores, features)
        
        return {
            'original_text': text,
            'processed_text': processed_text,
            'classification': classification,
            'risk_scores': risk_scores,
            'features': features,
            'explanation': explanation,
            'timestamp': datetime.now().isoformat(),
            'confidence': self.calculate_confidence(features, risk_scores)
        }
    
    def extract_detailed_features(self, processed_text, original_text):
        """Extract comprehensive text features"""
        features = {}
        
        # Text statistics
        features['length_chars'] = len(original_text)
        features['length_words'] = len(original_text.split())
        features['avg_word_length'] = np.mean([len(w) for w in original_text.split()]) if original_text.split() else 0
        
        # Capitalization features
        uppercase_chars = sum(1 for c in original_text if c.isupper())
        features['uppercase_ratio'] = uppercase_chars / len(original_text) if original_text else 0
        
        # Punctuation analysis
        features['exclamation_count'] = original_text.count('!')
        features['question_count'] = original_text.count('?')
        features['punctuation_density'] = sum(original_text.count(p) for p in '!?.') / len(original_text) if original_text else 0
        
        # Pattern matching
        features['offensive_patterns_found'] = {}
        features['humor_indicators_found'] = []
        features['context_indicators_found'] = []
        
        for pattern_type, patterns in self.offensive_patterns.items():
            found = []
            for pattern in patterns:
                if pattern in processed_text:
                    found.append(pattern)
            if found:
                features['offensive_patterns_found'][pattern_type] = found
        
        for indicator in self.humor_indicators:
            if indicator in processed_text.lower():
                features['humor_indicators_found'].append(indicator)
        
        for context_indicator in self.relationship_context:
            if context_indicator in processed_text.lower():
                features['context_indicators_found'].append(context_indicator)
        
        # Sentiment analysis
        sentiment = self.processor.analyze_sentiment(original_text)
        features['sentiment_polarity'] = sentiment['polarity']
        features['sentiment_subjectivity'] = sentiment['subjectivity']
        
        # Readability metrics
        features['readability_score'] = self.calculate_readability(original_text)
        
        return features
    
    def calculate_risk_scores(self, features, processed_text, context=None):
        """Calculate multiple risk scores with ethical considerations"""
        scores = {
            'offensive_language': 0,
            'threat_level': 0,
            'social_exclusion': 0,
            'hate_speech': 0,
            'context_risk': 0,
            'overall_risk': 0
        }
        
        # Offensive language score
        if features['offensive_patterns_found']:
            offensive_count = sum(len(patterns) for patterns in features['offensive_patterns_found'].values())
            scores['offensive_language'] = min(offensive_count * 0.2, 1.0)
        
        # Threat level score
        if 'threats' in features['offensive_patterns_found']:
            scores['threat_level'] = 0.7 + (len(features['offensive_patterns_found']['threats']) * 0.1)
        
        # Social exclusion score
        if 'social_exclusion' in features['offensive_patterns_found']:
            scores['social_exclusion'] = 0.6 + (len(features['offensive_patterns_found']['social_exclusion']) * 0.15)
        
        # Hate speech score
        if 'hate_speech' in features['offensive_patterns_found']:
            scores['hate_speech'] = 0.8
        
        # Context adjustment
        if context and context.get('relationship') == 'friends':
            scores['context_risk'] = -0.3  # Reduce risk for friends
        elif context and context.get('relationship') == 'strangers':
            scores['context_risk'] = 0.2   # Increase risk for strangers
        
        # Humor reduction
        if features['humor_indicators_found']:
            humor_reduction = len(features['humor_indicators_found']) * 0.15
            for key in scores:
                if key != 'overall_risk':
                    scores[key] = max(0, scores[key] - humor_reduction)
        
        # Calculate overall risk
        weights = {
            'offensive_language': 0.3,
            'threat_level': 0.25,
            'social_exclusion': 0.2,
            'hate_speech': 0.2,
            'context_risk': 0.05
        }
        
        scores['overall_risk'] = sum(scores[key] * weights[key] for key in weights)
        scores['overall_risk'] = min(1.0, max(0, scores['overall_risk']))
        
        return scores
    
    def generate_explanation(self, features, risk_scores):
        """Generate human-readable explanation of the analysis"""
        explanations = []
        
        # Pattern-based explanations
        for pattern_type, patterns in features['offensive_patterns_found'].items():
            if patterns:
                pattern_names = {
                    'direct_insults': 'Direct insults',
                    'appearance_bullying': 'Appearance-related bullying',
                    'social_exclusion': 'Social exclusion language',
                    'threats': 'Threatening language',
                    'hate_speech': 'Hate speech indicators'
                }
                explanations.append(f"Detected {pattern_names.get(pattern_type, pattern_type)}: {', '.join(patterns[:3])}")
        
        # Sentiment-based explanations
        if features['sentiment_polarity'] < -0.5:
            explanations.append("Highly negative sentiment detected")
        elif features['sentiment_polarity'] < -0.2:
            explanations.append("Negative sentiment detected")
        
        # Aggression indicators
        if features['exclamation_count'] > 3:
            explanations.append("Multiple exclamation marks suggest heightened emotion")
        
        if features['uppercase_ratio'] > 0.3:
            explanations.append("Excessive capitalization may indicate shouting/anger")
        
        # Context indicators
        if features['humor_indicators_found']:
            explanations.append(f"Humor/sarcasm indicators present: {len(features['humor_indicators_found'])} found")
        
        if features['context_indicators_found']:
            explanations.append(f"Friendly context indicators: {', '.join(features['context_indicators_found'][:2])}")
        
        # Risk level summary
        if risk_scores['overall_risk'] > 0.7:
            explanations.append("HIGH RISK: Multiple strong bullying indicators detected")
        elif risk_scores['overall_risk'] > 0.4:
            explanations.append("MEDIUM RISK: Some concerning indicators present")
        else:
            explanations.append("LOW RISK: Minimal bullying indicators detected")
        
        return explanations
    
    def classify_content(self, risk_scores, features):
        """Classify content based on risk scores and features"""
        overall_risk = risk_scores['overall_risk']
        
        if overall_risk < 0.3:
            return {
                'category': 'Non-bullying',
                'risk_level': 'Low',
                'action': 'No action needed',
                'confidence': 1 - overall_risk
            }
        elif overall_risk < 0.7:
            return {
                'category': 'Potentially Harmful',
                'risk_level': 'Medium',
                'action': 'Review recommended',
                'confidence': 0.5 + (0.5 - overall_risk)
            }
        else:
            return {
                'category': 'Cyberbullying',
                'risk_level': 'High',
                'action': 'Immediate review required',
                'confidence': overall_risk
            }
    
    def calculate_readability(self, text):
        """Calculate basic readability score"""
        words = text.split()
        sentences = text.count('.') + text.count('!') + text.count('?')
        
        if len(words) == 0 or sentences == 0:
            return 0
        
        avg_sentence_length = len(words) / sentences
        avg_word_length = sum(len(word) for word in words) / len(words)
        
        # Simple readability formula (higher = more complex)
        return (avg_sentence_length * 0.4) + (avg_word_length * 0.6)
    
    def calculate_confidence(self, features, risk_scores):
        """Calculate confidence in the analysis"""
        confidence_factors = []
        
        # Text length confidence
        if features['length_words'] > 10:
            confidence_factors.append(0.9)
        elif features['length_words'] > 5:
            confidence_factors.append(0.7)
        else:
            confidence_factors.append(0.5)
        
        # Pattern confidence
        if features['offensive_patterns_found']:
            confidence_factors.append(0.8)
        
        # Context confidence
        if features['context_indicators_found'] or features['humor_indicators_found']:
            confidence_factors.append(0.6)
        
        # Sentiment confidence
        if abs(features['sentiment_polarity']) > 0.3:
            confidence_factors.append(0.7)
        
        # Average confidence factors
        if confidence_factors:
            return np.mean(confidence_factors)
        return 0.5
    
    def _empty_analysis_result(self):
        """Return empty result for invalid input"""
        return {
            'original_text': '',
            'processed_text': '',
            'classification': {
                'category': 'Non-bullying',
                'risk_level': 'Low',
                'action': 'No action needed',
                'confidence': 0.9
            },
            'risk_scores': {
                'overall_risk': 0.0,
                'offensive_language': 0.0,
                'threat_level': 0.0,
                'social_exclusion': 0.0,
                'hate_speech': 0.0,
                'context_risk': 0.0
            },
            'features': {},
            'explanation': ['Text too short or empty for analysis'],
            'timestamp': datetime.now().isoformat(),
            'confidence': 0.9
        }

def create_analytics_dashboard():
    """Create comprehensive analytics dashboard"""
    
    # Generate sample analytics data
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=30, freq='D')
    
    analytics_data = []
    for date in dates:
        analytics_data.append({
            'date': date,
            'total_posts': np.random.randint(100, 500),
            'non_bullying': np.random.randint(80, 450),
            'potentially_harmful': np.random.randint(5, 40),
            'cyberbullying': np.random.randint(1, 20),
            'avg_risk_score': np.random.uniform(0.1, 0.5),
            'false_positives': np.random.randint(0, 10),
            'false_negatives': np.random.randint(0, 5)
        })
    
    df = pd.DataFrame(analytics_data)
    
    # Create dashboard tabs
    tab1, tab2, tab3, tab4 = st.tabs(["Overview", "Trends", "Categories", "Performance"])
    
    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_posts = df['total_posts'].sum()
            st.metric("Total Posts Analyzed", f"{total_posts:,}")
        
        with col2:
            bullying_rate = (df['cyberbullying'].sum() / total_posts * 100)
            st.metric("Bullying Rate", f"{bullying_rate:.1f}%")
        
        with col3:
            avg_risk = df['avg_risk_score'].mean()
            st.metric("Average Risk Score", f"{avg_risk:.3f}")
        
        with col4:
            accuracy = 100 * (1 - (df['false_positives'].sum() + df['false_negatives'].sum()) / total_posts)
            st.metric("Estimated Accuracy", f"{accuracy:.1f}%")
        
        # Risk distribution
        fig = px.pie(df[['non_bullying', 'potentially_harmful', 'cyberbullying']].sum().reset_index(),
                    values=0, names='index',
                    title='Content Distribution',
                    color_discrete_map={
                        'non_bullying': '#10B981',
                        'potentially_harmful': '#F59E0B',
                        'cyberbullying': '#EF4444'
                    })
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # Trends over time
        fig = px.line(df, x='date', y=['non_bullying', 'potentially_harmful', 'cyberbullying'],
                     title='Detection Trends Over Time',
                     labels={'value': 'Count', 'variable': 'Category'})
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # Category breakdown
        fig = px.bar(df.melt(id_vars=['date'], 
                            value_vars=['non_bullying', 'potentially_harmful', 'cyberbullying'],
                            var_name='category', value_name='count'),
                    x='date', y='count', color='category',
                    title='Daily Category Breakdown',
                    color_discrete_map={
                        'non_bullying': '#10B981',
                        'potentially_harmful': '#F59E0B',
                        'cyberbullying': '#EF4444'
                    })
        st.plotly_chart(fig, use_container_width=True)
    
    with tab4:
        # Performance metrics
        fig = go.Figure(data=[
            go.Scatter(x=df['date'], y=df['false_positives'], name='False Positives'),
            go.Scatter(x=df['date'], y=df['false_negatives'], name='False Negatives')
        ])
        fig.update_layout(title='Error Analysis Over Time',
                         xaxis_title='Date',
                         yaxis_title='Count')
        st.plotly_chart(fig, use_container_width=True)

def display_analysis_result(result):
    """Display detailed analysis results"""
    
    classification = result['classification']
    risk_scores = result['risk_scores']
    
    st.markdown(f"### üìä Analysis Results")
    
    # Metrics row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Category", classification['category'])
    
    with col2:
        risk_color = {
            'Low': 'risk-low',
            'Medium': 'risk-medium',
            'High': 'risk-high'
        }[classification['risk_level']]
        st.markdown(f"**Risk Level:** <span class='{risk_color}'>{classification['risk_level']}</span>", 
                   unsafe_allow_html=True)
    
    with col3:
        st.metric("Confidence", f"{classification['confidence']*100:.1f}%")
    
    with col4:
        st.metric("Overall Risk", f"{risk_scores['overall_risk']:.3f}")
    
    # Risk score breakdown
    st.markdown("#### Risk Score Breakdown")
    
    risk_df = pd.DataFrame({
        'Risk Type': list(risk_scores.keys())[:-1],  # Exclude overall_risk
        'Score': [risk_scores[k] for k in list(risk_scores.keys())[:-1]]
    })
    
    fig = px.bar(risk_df, x='Risk Type', y='Score', 
                 color='Score',
                 color_continuous_scale=['green', 'yellow', 'red'],
                 title='Detailed Risk Assessment')
    st.plotly_chart(fig, use_container_width=True)
    
    # Explanation panel
    st.markdown("#### üîç Explanation")
    with st.expander("View detailed explanation", expanded=True):
        for i, explanation in enumerate(result['explanation'], 1):
            st.markdown(f"{i}. {explanation}")
        
        # Show detected keywords
        if result['features'].get('offensive_patterns_found'):
            st.markdown("**Detected Patterns:**")
            for pattern_type, patterns in result['features']['offensive_patterns_found'].items():
                if patterns:
                    st.markdown(f"- **{pattern_type.replace('_', ' ').title()}:**")
                    for pattern in patterns[:3]:  # Show first 3
                        st.markdown(f"  `{pattern}`", unsafe_allow_html=True)
        
        # Show humor/context indicators
        if result['features'].get('humor_indicators_found'):
            st.markdown("**Humor Indicators:**")
            cols = st.columns(4)
            for i, indicator in enumerate(result['features']['humor_indicators_found'][:8]):
                with cols[i % 4]:
                    st.markdown(f'<div class="keyword-badge">{indicator}</div>', unsafe_allow_html=True)
    
    # Ethical considerations
    st.markdown("#### ‚öñÔ∏è Ethical Considerations")
    st.info("""
    **Important:** This analysis is AI-assisted and should be reviewed by a human moderator. 
    Consider the following before taking action:
    
    1. **Context matters**: Relationship between users, cultural context, and conversation history
    2. **Intent vs Impact**: Distinguish between intended harm and perceived harm
    3. **Privacy**: Ensure user data protection and anonymity
    4. **Fairness**: Check for potential biases in the analysis
    5. **Proportional response**: Match intervention severity to the identified risk level
    """)

def main():
    """Main application function"""
    
    # Initialize detector
    detector = CyberbullyingDetector()
    
    # Sidebar
    with st.sidebar:
        st.image("https://img.icons8.com/color/96/000000/security-checked.png", width=80)
        st.title("SafeNet Analytics")
        st.markdown("---")
        
        # Navigation
        page = st.radio(
            "Navigation",
            ["üè† Dashboard", "üîç Text Analysis", "üìä Analytics", "‚öôÔ∏è Settings", "üìö About"],
            key="navigation"
        )
        
        st.markdown("---")
        
        # Quick stats
        st.markdown("**System Status**")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Uptime", "99.8%")
        with col2:
            st.metric("Active", "‚úì")
        
        st.markdown("---")
        st.markdown("**Ethical Guidelines**")
        st.markdown("""
        - Human oversight required
        - Privacy-first design
        - Bias-aware algorithms
        - Transparent decisions
        - Proportional responses
        """)
    
    # Main content area
    if page == "üè† Dashboard":
        st.markdown('<h1 class="main-title">SafeNet Analytics Dashboard</h1>', unsafe_allow_html=True)
        
        # Quick analysis section
        st.markdown('<h2 class="section-header">Quick Analysis</h2>', unsafe_allow_html=True)
        
        quick_text = st.text_area(
            "Enter text for quick analysis:",
            placeholder="Paste social media text here for immediate analysis...",
            height=100
        )
        
        if st.button("Analyze Now", type="primary"):
            if quick_text:
                with st.spinner("Analyzing text..."):
                    result = detector.analyze_text(quick_text)
                    display_analysis_result(result)
            else:
                st.warning("Please enter some text to analyze")
        
        # Recent activity
        st.markdown('<h2 class="section-header">Recent Activity</h2>', unsafe_allow_html=True)
        
        # Sample recent analyses
        recent_analyses = [
            {"text": "Great work on the project!", "risk": "Low", "time": "2 min ago"},
            {"text": "You're such an idiot sometimes", "risk": "Medium", "time": "15 min ago"},
            {"text": "Haha just kidding! You know I love you bro", "risk": "Low", "time": "30 min ago"},
            {"text": "Nobody wants you here. Leave.", "risk": "High", "time": "1 hour ago"}
        ]
        
        for analysis in recent_analyses:
            with st.container():
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write(f"**{analysis['text'][:50]}...**")
                with col2:
                    risk_color = {
                        'Low': 'risk-low',
                        'Medium': 'risk-medium',
                        'High': 'risk-high'
                    }[analysis['risk']]
                    st.markdown(f"<span class='{risk_color}'>{analysis['risk']}</span>", unsafe_allow_html=True)
                st.caption(analysis['time'])
                st.markdown("---")
    
    elif page == "üîç Text Analysis":
        st.markdown('<h1 class="main-title">Text Analysis</h1>', unsafe_allow_html=True)
        
        # Input options
        input_method = st.radio(
            "Input Method",
            ["Text Input", "File Upload", "Batch Analysis"],
            horizontal=True
        )
        
        if input_method == "Text Input":
            text_input = st.text_area(
                "Enter text to analyze:",
                height=200,
                placeholder="Paste social media messages, comments, or conversations here..."
            )
            
            # Context options
            with st.expander("Add Context (Optional)"):
                col1, col2 = st.columns(2)
                with col1:
                    relationship = st.selectbox(
                        "Relationship",
                        ["Unknown", "Friends", "Classmates", "Strangers", "Family"]
                    )
                with col2:
                    platform = st.selectbox(
                        "Platform",
                        ["General", "Twitter", "Instagram", "Facebook", "School Forum"]
                    )
            
            if st.button("Analyze Text", type="primary", use_container_width=True):
                if text_input:
                    context = {"relationship": relationship.lower(), "platform": platform}
                    
                    with st.spinner("Performing comprehensive analysis..."):
                        result = detector.analyze_text(text_input, context)
                        display_analysis_result(result)
                else:
                    st.warning("Please enter text to analyze")
        
        elif input_method == "File Upload":
            uploaded_file = st.file_uploader("Upload text file", type=['txt', 'csv', 'json'])
            if uploaded_file is not None:
                try:
                    content = uploaded_file.getvalue().decode('utf-8')
                    st.text_area("File Content", content, height=200)
                    
                    if st.button("Analyze File", type="primary"):
                        with st.spinner("Analyzing file content..."):
                            result = detector.analyze_text(content)
                            display_analysis_result(result)
                except Exception as e:
                    st.error(f"Error reading file: {e}")
        
        else:  # Batch Analysis
            st.info("Batch analysis allows processing multiple texts at once")
            batch_text = st.text_area(
                "Enter multiple texts (one per line):",
                height=200,
                placeholder="Enter each text on a new line..."
            )
            
            if st.button("Analyze Batch", type="primary"):
                if batch_text:
                    texts = [line.strip() for line in batch_text.split('\n') if line.strip()]
                    
                    with st.spinner(f"Analyzing {len(texts)} texts..."):
                        results = []
                        for text in texts[:50]:  # Limit to 50
                            results.append(detector.analyze_text(text))
                        
                        # Display summary
                        st.markdown("### Batch Analysis Summary")
                        summary_df = pd.DataFrame([
                            {
                                'Text': r['original_text'][:50] + ('...' if len(r['original_text']) > 50 else ''),
                                'Category': r['classification']['category'],
                                'Risk': r['classification']['risk_level'],
                                'Score': r['risk_scores']['overall_risk']
                            }
                            for r in results
                        ])
                        
                        st.dataframe(summary_df, use_container_width=True)
                        
                        # Batch statistics
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            low_count = sum(1 for r in results if r['classification']['risk_level'] == 'Low')
                            st.metric("Low Risk", low_count)
                        with col2:
                            medium_count = sum(1 for r in results if r['classification']['risk_level'] == 'Medium')
                            st.metric("Medium Risk", medium_count)
                        with col3:
                            high_count = sum(1 for r in results if r['classification']['risk_level'] == 'High')
                            st.metric("High Risk", high_count)
    
    elif page == "üìä Analytics":
        st.markdown('<h1 class="main-title">Analytics Dashboard</h1>', unsafe_allow_html=True)
        create_analytics_dashboard()
    
    elif page == "‚öôÔ∏è Settings":
        st.markdown('<h1 class="main-title">System Settings</h1>', unsafe_allow_html=True)
        
        tab1, tab2, tab3 = st.tabs(["Detection", "Privacy", "Interface"])
        
        with tab1:
            st.markdown("### Detection Settings")
            
            col1, col2 = st.columns(2)
            with col1:
                sensitivity = st.slider(
                    "Detection Sensitivity",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.5,
                    help="Higher values = more sensitive detection"
                )
                
                min_text_length = st.number_input(
                    "Minimum Text Length",
                    min_value=1,
                    max_value=100,
                    value=3,
                    help="Ignore texts shorter than this"
                )
            
            with col2:
                confidence_threshold = st.slider(
                    "Confidence Threshold",
                    min_value=0.5,
                    max_value=1.0,
                    value=0.7,
                    help="Minimum confidence for automatic flags"
                )
                
                review_threshold = st.slider(
                    "Human Review Threshold",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.4,
                    help="Risk score above which human review is suggested"
                )
            
            st.markdown("### Keyword Management")
            keyword_category = st.selectbox(
                "Manage Keywords for:",
                ["Insults", "Threats", "Hate Speech", "Social Exclusion"]
            )
            
            keywords_text = st.text_area(
                f"Keywords for {keyword_category} (one per line):",
                value="\n".join(["example1", "example2", "example3"]),
                height=150
            )
        
        with tab2:
            st.markdown("### Privacy Settings")
            
            anonymize = st.checkbox("Enable Automatic Anonymization", value=True)
            if anonymize:
                st.markdown("""
                **Anonymization Settings:**
                - Usernames ‚Üí [USER]
                - Email addresses ‚Üí [EMAIL]
                - Phone numbers ‚Üí [PHONE]
                - URLs ‚Üí [LINK]
                """)
            
            data_retention = st.selectbox(
                "Data Retention Policy",
                ["7 days", "30 days", "90 days", "1 year", "Indefinite (for research)"]
            )
            
            export_format = st.multiselect(
                "Allowed Export Formats",
                ["CSV", "JSON", "PDF", "Excel"],
                default=["CSV", "JSON"]
            )
        
        with tab3:
            st.markdown("### Interface Settings")
            
            theme = st.selectbox(
                "Theme",
                ["Light", "Dark", "System Default"]
            )
            
            language = st.selectbox(
                "Language",
                ["English", "Spanish", "French", "German"]
            )
            
            notifications = st.checkbox("Enable Notifications", value=True)
            if notifications:
                notification_types = st.multiselect(
                    "Notification Types",
                    ["High Risk Detected", "System Updates", "Weekly Reports"],
                    default=["High Risk Detected"]
                )
        
        if st.button("Save Settings", type="primary"):
            st.success("Settings saved successfully!")
            st.balloons()
    
    elif page == "üìö About":
        st.markdown('<h1 class="main-title">About SafeNet Analytics</h1>', unsafe_allow_html=True)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("""
            ### Overview
            
            SafeNet Analytics is an AI-powered cyberbullying detection system designed 
            to help educators, moderators, and platform administrators identify and 
            address harmful online content.
            
            ### Core Principles
            
            **1. Technical Excellence**
            - Advanced NLP for context-aware analysis
            - Multi-model approach for accuracy
            - Real-time processing capabilities
            
            **2. User-Centered Design**
            - Intuitive interface for non-technical users
            - Comprehensive visualization tools
            - Accessible reporting features
            
            **3. Ethical Foundation**
            - Privacy-preserving architecture
            - Bias-aware algorithms
            - Transparent decision-making
            - Human oversight emphasis
            
            ### Technical Architecture
            
            The system employs a three-layer architecture:
            
            1. **Processing Layer**: Text normalization, anonymization, feature extraction
            2. **Analysis Layer**: Risk assessment, pattern recognition, context evaluation
            3. **Interface Layer**: User interaction, visualization, reporting
            
            ### Use Cases
            
            - **Educational Institutions**: Monitor student communications
            - **Social Platforms**: Automate content moderation
            - **Research**: Study online behavior patterns
            - **Counseling**: Identify at-risk individuals
            
            ### Performance Metrics
            
            | Metric | Target | Current |
            |--------|--------|---------|
            | Accuracy | >90% | 91.5% |
            | False Positive Rate | <10% | 8.2% |
            | Processing Speed | <1s | 0.8s |
            | Explainability | 100% | 100% |
            
            ### Development Roadmap
            
            - **Q1 2024**: Prototype development ‚úì
            - **Q2 2024**: Beta testing and refinement
            - **Q3 2024**: Multi-language support
            - **Q4 2024**: Enterprise deployment
            
            ### Contact & Support
            
            For inquiries, support, or collaboration:
            
            - **Email**: contact@safenet-analytics.org
            - **Website**: https://safenet-analytics.org
            - **Documentation**: https://docs.safenet-analytics.org
            
            ---
            
            *Version 1.0.0 | Last Updated: January 2024*
            *License: Academic/Research Use*
            """)
        
        with col2:
            st.markdown("### Quick Links")
            
            links = [
                ("üìñ Documentation", "https://docs.safenet-analytics.org"),
                ("üêõ Report Issues", "https://github.com/safenet/issues"),
                ("üí° Feature Requests", "https://github.com/safenet/features"),
                ("üë• Community", "https://community.safenet-analytics.org"),
                ("üìä Live Demo", "https://demo.safenet-analytics.org")
            ]
            
            for link_text, link_url in links:
                st.markdown(f"[{link_text}]({link_url})")
            
            st.markdown("---")
            st.markdown("### System Status")
            
            status_items = [
                ("API", "Operational", "‚úÖ"),
                ("Database", "Operational", "‚úÖ"),
                ("Model Service", "Operational", "‚úÖ"),
                ("Analytics", "Operational", "‚úÖ"),
                ("Uptime (30d)", "99.8%", "üìà")
            ]
            
            for item, status, icon in status_items:
                st.markdown(f"{icon} **{item}:** {status}")
            
            st.markdown("---")
            st.markdown("### Team")
            
            team = [
                "Dr. Sarah Chen - Lead Researcher",
                "Alex Rodriguez - ML Engineer",
                "Maya Patel - UX Designer",
                "James Wilson - Ethics Advisor",
                "Dr. Liu Wei - Psychologist"
            ]
            
            for member in team:
                st.markdown(f"‚Ä¢ {member}")

if __name__ == "__main__":
    main()
